{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youssefokeil/HiFT-reimplementation/blob/master/HiFT_reimplementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76mDtudI7E29"
      },
      "source": [
        "# Hierarchical Feature Transformer Reimplementation\n",
        "\n",
        "In this notebook, we’ll *recreate* a Visual Tracking method that is outlined in the paper, [HiFT: Hierarchical Feature Transformer for Aerial Tracking](https://ieeexplore.ieee.org/document/9710895) in PyTorch.\n",
        "\n",
        "In this paper, visual tracking uses the features extracted from an AlexNet, which is comprised of a series of convolutional and pooling layers, and a few fully-connected layers.\n",
        "\n",
        "<img src='https://github.com/youssefokeil/HiFT-reimplementation/blob/master/Notebook_Images/HiFT-overview.png?raw=true' width=80% />\n",
        "\n",
        "### The Hierarchical Feature Transformer\n",
        "\n",
        "Using the transformer layer, we use the extracted features from AlexNet to make a high-resolution feature encoding and low-resolution feature decoding. The  high-resolution encoding layer is to learn interdependencies between different feature layers and to raise attention to objkect with different scales. The low-resolution decoder gets the semantic informatiion from  the low-level feature map.\n",
        "\n",
        "The model then uses a regression and a classification network as the head of the model. Looking at the image below, you can understand more the approach of the paper."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the requirements specified in the github repo of the research group."
      ],
      "metadata": {
        "id": "XnO88KA-C6LZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python yacs tqdm colorama cython;"
      ],
      "metadata": {
        "id": "mgbgJVSNDO-o",
        "outputId": "766bfc5f-bbf6-430b-a04c-17d3a6e699ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Collecting yacs\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from yacs) (6.0.2)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: yacs, colorama\n",
            "Successfully installed colorama-0.4.6 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note to self: From the resources of the HiFT, pyyaml is old and I'm not sure if we have to use it."
      ],
      "metadata": {
        "id": "GkrCB2nYEo8J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "Ceixg6AS7E2-"
      },
      "outputs": [],
      "source": [
        "# import resources\n",
        "%matplotlib inline\n",
        "\n",
        "# import pyyaml\n",
        "import yacs\n",
        "import tqdm\n",
        "import colorama\n",
        "import cython\n",
        "#import tensorboardX\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import requests\n",
        "from torchvision import transforms, models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QUNU2X07E2_"
      },
      "source": [
        "## The AlexNet Backbone\n",
        "\n",
        "While the AlexNet can be found anywhere, we'll write it ourselves for (mostly) learning purposes and as we can change our model and try different architecture, it's better if we see the configuration ourselves."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a glimpse at the AlexNet architecture, we'll only need the convolutional and pooling layers. Also, following the source code of the *HiFT* we may use batchnorm layers between each convolutional stack.\n",
        "\n",
        "<img src='https://upload.wikimedia.org/wikipedia/commons/c/cc/Comparison_image_neural_networks.svg' width=80% />\n"
      ],
      "metadata": {
        "id": "_e1HJh3BIz75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(AlexNet, self).__init__()\n",
        "\n",
        "      ## we'll not use  batchnorm and then we'll try it\n",
        "\n",
        "      self.layer1 = nn.Sequential(\n",
        "          nn.Conv2d(3,96,kernel_size=11, stride=2),\n",
        "          nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "\n",
        "      self.layer2 = nn.Sequential(\n",
        "          nn.Conv2d(96, 256, kernel_size=5, stride=2),\n",
        "          nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "\n",
        "      self.layer3 = nn.Sequential(\n",
        "          nn.Conv2d(256, 384, kernel_size=3, stride=1),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "\n",
        "      self.layer4 = nn.Sequential(\n",
        "          nn.Conv2d(384, 384, kernel_size=3, stride=1),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "\n",
        "      self.layer5 = nn.Sequential(\n",
        "          nn.Conv2d(384, 256, kernel_size=3, stride=1)\n",
        "      )\n",
        "\n",
        "      def forward(self, x):\n",
        "        x=self.layer1(x)\n",
        "        x=self.layer2(x)\n",
        "        x=self.layer3(x)\n",
        "        x=self.layer4(x)\n",
        "        x=self.layer5(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "1BhCJbmZJtug"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check which device you're running. Make sure it's `CUDA`"
      ],
      "metadata": {
        "id": "7cyZlsjZUSOm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uj997Stl7E2_",
        "outputId": "49b45401-6ccc-401e-9aec-e75be01156f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# move the model to GPU, if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95oYIqDo7E3A"
      },
      "source": [
        "### Transformer Implementation\n",
        "\n",
        "Looking at the paper, they use a different implementation of the transformer which demands that for me to understand transformers pretty well. Or just copy paste the code. I'll choose the former,"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODOS: Hierarchical Transformer definition"
      ],
      "metadata": {
        "id": "BbTFbIVIzNmR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression (Localization) & Classification Network Head"
      ],
      "metadata": {
        "id": "d0Okf7YOaAt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define the regression and classification heads. The point of this network is:\n",
        "\n",
        "\n",
        "*   **The regression network**; predicts where the object is located. It does this by outputting 4 values that define the bounding box parameters.\n",
        "*   **The classification network**; determines if the object is present at each location. Outputs a probability score of the confidence it has of the presence of the object.\n",
        "\n",
        "We combine both high classification scores & the accurate localization to achieve successful tracking."
      ],
      "metadata": {
        "id": "MceYLEjVaK4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the implementation of the paper, they used two classification labels\n",
        "\n",
        "\n",
        "> To achieve accurate\n",
        "classiﬁcation, we apply two classiﬁcation branches. One\n",
        "branch aims to classify via the area involved in the ground\n",
        "truth box. The other branch focuses on determining the positive samples measured by the distance between the center\n",
        "of ground truth and the corresponding point.\n",
        "\n",
        " *HiFT 2021*\n",
        "\n"
      ],
      "metadata": {
        "id": "rQI1aZa1t_xR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelHead(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(ModelHead, self).__init__()\n",
        "\n",
        "      # uses \"same convolution\" for spatial awareness & to learn some edges\n",
        "      self.conv_layer1=nn.Sequential(\n",
        "          nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
        "          nn.BatchNorm2d(192),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "\n",
        "      # outputs 4 values corresponding to the bounding box\n",
        "      self.layer_loc = nn.Conv2d(192, 4)\n",
        "\n",
        "      self.cls1 = nn.Conv2d(192, 2, kernel_size=3, stride=1, padding=1)\n",
        "      self.cls2 = nn.Conv2d(192, 1, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "        # notice that you can call layer 1 multiple times, in the implementation\n",
        "        # of the paper they called it 3 times, now we can experiment with\n",
        "        # different calls, now let's call only once\n",
        "        loc = self.conv_layer1(x)\n",
        "        loc = self.layer_loc(loc)\n",
        "\n",
        "        cls = self.layer1(x)\n",
        "\n",
        "        # first classification branch\n",
        "        cls1=self.cls1(cls)\n",
        "\n",
        "        #second classification branch\n",
        "        cls2=self.cls2(cls)\n",
        "\n",
        "\n",
        "        return loc, cls1, cls2"
      ],
      "metadata": {
        "id": "hEJwGRgPhCli"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function"
      ],
      "metadata": {
        "id": "HNWHpYL_v7Hk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the output of the function has three values. The loss is a mix of the the three outputs.\n",
        "\n",
        "\n",
        "\n",
        "\\ Loverall = λ1 Lcls1 + λ2 Lcls2 + λ3 Lloc\n",
        "\n",
        "> where Lcls1 , Lcls2 , Lloc represent the cross-entropy, binary\n",
        "cross-entropy, and IoU loss. λ1 , λ2 , and λ3 are the coefﬁ-\n",
        "cients to balance the contributions of each loss\n"
      ],
      "metadata": {
        "id": "DYPmeeQav-0N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LmjJyAf5v-dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "9IoEKCWn7E3D"
      },
      "source": [
        "## Putting it all Together\n",
        "\n",
        "Now let's put all the elements together and train our model"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}